# -*- coding: utf-8 -*-
"""AI_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hmo6BQZmgD5h1UlL74xXpdS1CuYOZlQz
"""

!pip install transformers sentence-transformers

from sentence_transformers import SentenceTransformer, util

faq = {
    "latest smartphone specs": "Our latest smartphone features a 6.5-inch OLED display, 128GB storage, and a 48MP camera.",
    "order tracking": "You can track your order using the tracking link sent to your email after purchase.",
    "return policy": "Returns accepted within 30 days with the original receipt and packaging.",
    "payment methods": "We accept credit cards, debit cards, PayPal, and Apple Pay.",
    "warranty": "All products come with a 1-year limited warranty covering manufacturing defects."
}

embedder = SentenceTransformer('all-MiniLM-L6-v2')

faq_questions = list(faq.keys())
faq_embeddings = embedder.encode(faq_questions, convert_to_tensor=True)

def find_best_faq_answer(user_query):
    query_embedding = embedder.encode(user_query, convert_to_tensor=True)
    hits = util.semantic_search(query_embedding, faq_embeddings, top_k=1)
    best_match_idx = hits[0][0]['corpus_id']
    score = hits[0][0]['score']
    if score > 0.5:
        return faq[faq_questions[best_match_idx]]
    return None

tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-small")
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-small")

# Store chat history IDs for multi-turn conversation
chat_history_ids = None

print("Chatbot is ready! Type 'exit' or 'quit' to stop.")

while True:
    user_input = input("You: ")
    if user_input.lower() in ["exit", "quit"]:
        print("Bot: Goodbye!")
        break

    # Check FAQ first
    faq_answer = find_best_faq_answer(user_input)
    if faq_answer:
        print("Bot (FAQ):", faq_answer)
        continue

    # If no FAQ answer, use the LLM conversational model
    # Encode user input + chat history
    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')
    if chat_history_ids is not None:
        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)
    else:
        bot_input_ids = new_input_ids

    # Generate response
    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)
    bot_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)

    print("Bot:", bot_response)
